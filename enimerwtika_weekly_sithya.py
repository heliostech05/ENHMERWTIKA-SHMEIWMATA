#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
enimerwtika_weekly_sithya.py

Î•Î²Î´Î¿Î¼Î±Î´Î¹Î±Î¯Î± ÎµÎ½Î·Î¼ÎµÏÏ‰Ï„Î¹ÎºÎ¬ ÎœÎŸÎÎŸ Î³Î¹Î± Î£Î—Î˜Î¥Î‘.

Î›ÎŸÎ“Î™ÎšÎ—
------
- Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯ Î‘ÎšÎ¡Î™Î’Î©Î£ Ï„Î· Î»Î¿Î³Î¹ÎºÎ® Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼Î¿Ï Ï„Î¿Ï… Î¼Î·Î½Î¹Î±Î¯Î¿Ï… script
  (pairing 15Î»Î­Ï€Ï„Ï‰Î½ Ï€Î±ÏÎ±Î³Ï‰Î³Î®Ï‚â€“DAM, DST, Ï€ÏÎ¬Î¾ÎµÎ¹Ï‚ Îº.Î»Ï€.) Î¼Î­ÏƒÏ‰ Ï„Î·Ï‚
  ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ·Ï‚ calculate_daily_summary_quarterly, Î· Î¿Ï€Î¿Î¯Î± ÎµÎ´Ï
  Î­Ï‡ÎµÎ¹ Î±Î½Ï„Î¹Î³ÏÎ±Ï†ÎµÎ¯ Î±Ï€ÏŒ Ï„Î¿ Î¼Î·Î½Î¹Î±Î¯Î¿.

- Î“Î¹Î± ÎºÎ¬Î¸Îµ Ï€Î±ÏÎ±Î³Ï‰Î³ÏŒ Î£Î—Î˜Î¥Î‘:
    1) Î¤ÏÎ­Ï‡Î¿Ï…Î¼Îµ Ï„Î· Î¼Î·Î½Î¹Î±Î¯Î± calculate_daily_summary_quarterly
       Î³Î¹Î± Î¿Î»ÏŒÎºÎ»Î·ÏÎ¿ Ï„Î¿Î½ Î¼Î®Î½Î±.
    2) Î¦Î¹Î»Ï„ÏÎ¬ÏÎ¿Ï…Î¼Îµ Ï„Î¹Ï‚ Î·Î¼ÎµÏÎ¿Î¼Î·Î½Î¯ÎµÏ‚ Î¼ÏŒÎ½Î¿ ÏƒÏ„Î¿ Î´Î¹Î¬ÏƒÏ„Î·Î¼Î±
       [start_date .. end_date].
    3) ÎÎ±Î½Î±Ï‹Ï€Î¿Î»Î¿Î³Î¯Î¶Î¿Ï…Î¼Îµ Ï„Î± ÏƒÏÎ½Î¿Î»Î± Î¼ÏŒÎ½Î¿ Î³Î¹Î± Î±Ï…Ï„Î­Ï‚ Ï„Î¹Ï‚ Î¼Î­ÏÎµÏ‚.
    4) Î¦Ï„Î¹Î¬Ï‡Î½Î¿Ï…Î¼Îµ Excel/PDF Î¼Îµ ÎµÎ²Î´Î¿Î¼Î±Î´Î¹Î±Î¯Î¿ Ï„Î¯Ï„Î»Î¿.

- Î Î¡ÎŸÎ£ÎŸÎ§Î—: Î— ÎµÎ²Î´Î¿Î¼Î¬Î´Î± Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± ÎµÎ¯Î½Î±Î¹ Î¼Î­ÏƒÎ± ÏƒÏ„Î¿Î½ Î¯Î´Î¹Î¿ Î¼Î®Î½Î±.

Î‘Ï€Î±Î¹Ï„ÎµÎ¯:
- ADMIE_MERGED_TIMOLOGIA.py (Ï„Î¿ Î¼Î·Î½Î¹Î±Î¯Î¿) Î”Î•Î Ï‡ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹ Î½Î± Î³Î¯Î½ÎµÎ¹ import.
  Î•Î´Ï Î­Ï‡Î¿Ï…Î½ Î±Î½Ï„Î¹Î³ÏÎ±Ï†ÎµÎ¯ Î¿Î¹ Î±Ï€Î±ÏÎ±Î¯Ï„Î·Ï„ÎµÏ‚ ÏƒÏ…Î½Î±ÏÏ„Î®ÏƒÎµÎ¹Ï‚ (DAM, Ï€Î±ÏÎ±Î³Ï‰Î³Î®, calculation).
"""

import os
import re
import shutil
import subprocess
import time
from pathlib import Path
from collections import defaultdict
import unicodedata
import pandas as pd

from openpyxl import load_workbook
from openpyxl.styles import Font, Alignment
from openpyxl.cell.cell import MergedCell
from openpyxl.utils.cell import coordinate_to_tuple
from openpyxl.worksheet.worksheet import Worksheet

# Î ÏÎ¿Î±Î¹ÏÎµÏ„Î¹ÎºÎ¬ Î³Î¹Î± PDF Î¼Î­ÏƒÏ‰ Excel
try:
    import xlwings as xw  # noqa
    _HAS_XLWINGS = True
except Exception:
    _HAS_XLWINGS = False

# =================== Paths / Globals ===================

BASE_DIR = Path(__file__).resolve().parent

TEMPLATE_FILE   = BASE_DIR / "WEEKLY_Invoice_GREEN_VALUE_01.xlsx"
PRODUCERS_XLSX  = BASE_DIR / "producers.xlsx"
DAM_FILE_2025   = BASE_DIR / "energy-charts_Electricity_production_and_spot_prices_in_Greece_in_2026.csv"

PROD_DIR        = BASE_DIR / "Î Î‘Î¡Î‘Î“Î©Î“Î—"
DOWNLOADS_DIR   = BASE_DIR / "downloads"   # downloads/<YYYY-MM>/GREEN_VE6*.csv

LOG_BASE        = BASE_DIR / "logs" / "timologia_weekly"
LOG_BASE.mkdir(parents=True, exist_ok=True)

MAX_FOLDER_CHARS   = 120
MAX_FILENAME_CHARS = 140

WIN_RESERVED = {
    "CON","PRN","AUX","NUL",
    "COM1","COM2","COM3","COM4","COM5","COM6","COM7","COM8","COM9",
    "LPT1","LPT2","LPT3","LPT4","LPT5","LPT6","LPT7","LPT8","LPT9"
}


def log(name, msg):
    with open(LOG_BASE / f"{name}.txt", "a", encoding="utf-8") as f:
        f.write(str(msg) + "\n")


# =================== Helpers (names/paths) ===================

def sanitize_name(name: str) -> str:
    if name is None:
        return "UNTITLED"
    s = str(name)
    s = re.sub(r'[\\/*?:"<>|]', "", s)
    s = s.strip().rstrip(".")
    if not s:
        s = "UNTITLED"
    if s.upper() in WIN_RESERVED:
        s = "_" + s
    return s

def normalize_name(name: str) -> str:
    return re.sub(r'[\\._\\-\\s]', '', str(name).strip().lower())

def join_with_limit(parts, sep=" & ", limit=120):
    out, total = [], 0
    for p in parts:
        piece = p if not out else (sep + p)
        if total + len(piece) > limit:
            break
        out.append(p)
        total += len(piece)
    if out:
        return sep.join(out)
    return (parts[0] if parts else "UNTITLED")[:limit]

def clipped_folder_name(preferred_names, fallback_names, limit=MAX_FOLDER_CHARS):
    def build_name(items):
        uniq = sorted({sanitize_name(x) for x in items if x})
        return join_with_limit(uniq, sep=" & ", limit=limit)
    if preferred_names:
        return build_name(preferred_names)
    return build_name(fallback_names)

def clipped_filename_weekly(company_name: str, tag: str, ext: str, limit=MAX_FILENAME_CHARS):
    prefix = "Î•Î’Î”ÎŸÎœÎ‘Î”Î™Î‘Î™ÎŸ_Î£Î—ÎœÎ•Î™Î©ÎœÎ‘_"
    base = sanitize_name(company_name).replace(" ", "_")
    cand = f"{prefix}{base}_{tag}.{ext}"
    if len(cand) <= limit:
        return cand
    over = len(cand) - limit
    base_cut = base[:max(1, len(base) - over)]
    cand = f"{prefix}{base_cut}_{tag}.{ext}"
    if len(cand) > limit:
        trunk = f"{prefix}{tag}"
        cand = trunk[:limit - (len(ext) + 1)] + "." + ext
    return cand

def xlsx_filename_weekly(company_name, tag):
    return clipped_filename_weekly(company_name, tag, "xlsx", MAX_FILENAME_CHARS)

def pdf_filename_weekly(company_name, tag):
    return clipped_filename_weekly(company_name, tag, "pdf", MAX_FILENAME_CHARS)


# =================== Producers (SITHYA only) ===================

def load_producers_sithya(filepath=PRODUCERS_XLSX):
    fn = "load_producers_sithya"
    try:
        if not Path(filepath).exists():
            log(fn, f"âŒ producers.xlsx not found: {filepath}")
            return None
        df = pd.read_excel(filepath, dtype={'Code': str})

        for col in ['Email','IBAN','Code']:
            if col not in df.columns:
                df[col] = ""
        needed = [
            'Î•Ï„Î±Î¹ÏÎµÎ¯Î±','Email','ÎœÎ¿Î½Î±Î´Î¹Î±Î¯Î± Î§ÏÎ­Ï‰ÏƒÎ· Î¦Î¿Î£Î•',
            'Î‘.Îœ. Î‘Î Î•','Î‘Î¦Îœ','Î”ÎŸÎ¥','Î”Î¹ÎµÏÎ¸Ï…Î½ÏƒÎ·','Î¤ÎµÏ‡Î½Î¿Î»Î¿Î³Î¯Î±','IBAN','Code'
        ]
        missing = [c for c in needed if c not in df.columns]
        if missing:
            raise ValueError(f"Î›ÎµÎ¯Ï€Î¿Ï…Î½ ÏƒÏ„Î®Î»ÎµÏ‚: {missing}")

        tech = df['Î¤ÎµÏ‡Î½Î¿Î»Î¿Î³Î¯Î±'].astype(str).str.strip().str.upper()
        aliases = {"Î£Î—Î˜Î¥Î‘","Î£Î—Î˜","Î£Î—Î˜Î¥Î‘/Î£Î—Î˜","Î£Î—Î˜Î¥Î‘ - CHP","CHP","Î£Î—Î˜-YA"}
        mask = tech.isin(aliases) | tech.str.contains(r"\bÎ£Î—Î˜Î¥Î‘\b", regex=True)
        df = df[mask].copy()

        if df.empty:
            log(fn, "âš ï¸ Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ Ï€Î±ÏÎ±Î³Ï‰Î³Î¿Î¯ Î£Î—Î˜Î¥Î‘.")
            return df

        df['normalized_name'] = df['Î•Ï„Î±Î¹ÏÎµÎ¯Î±'].astype(str).apply(normalize_name)
        # Also produce a per-producer log file so we have one document per company
        try:
            for _, prow in df.iterrows():
                comp_name = str(prow.get('Î•Ï„Î±Î¹ÏÎµÎ¯Î±', '') or '').strip()
                if not comp_name:
                    continue
                safe = normalize_name(comp_name)
                try:
                    log(f"producer_{safe}", prow.to_dict())
                except Exception as e:
                    log(fn, f"failed to write individual log for {comp_name}: {e}")
        except Exception as e:
            log(fn, f"failed to produce per-producer logs: {e}")
        log(fn, f"OK Î£Î—Î˜Î¥Î‘ producers: {len(df)}")
        return df
    except Exception as e:
        log(fn, f"ERROR {e}")
        return None

def build_email_groups(producers_df):
    email_to_companies = defaultdict(set)
    for _, row in producers_df.iterrows():
        email = (str(row.get('Email', '') or '').strip()) or "NO_EMAIL"
        comp  = str(row.get('Î•Ï„Î±Î¹ÏÎµÎ¯Î±', '') or '').strip()
        if comp:
            email_to_companies[email].add(comp)
    email_to_companies = {em: sorted(v) for em, v in email_to_companies.items()}
    return email_to_companies, {}


# =================== GREEN_VE6 â†’ Î Î‘Î¡Î‘Î“Î©Î“Î— ===================

def load_producers_basic(filepath=PRODUCERS_XLSX):
    fn = "load_producers_basic"
    try:
        if not Path(filepath).exists():
            log(fn, f"âŒ producers.xlsx not found: {filepath}")
            return None
        df = pd.read_excel(filepath, dtype={'Code': str})
        if 'Code' not in df.columns or 'Î•Ï„Î±Î¹ÏÎµÎ¯Î±' not in df.columns:
            raise ValueError("Î¤Î¿ producers.xlsx Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± Î­Ï‡ÎµÎ¹ 'Code' ÎºÎ±Î¹ 'Î•Ï„Î±Î¹ÏÎµÎ¯Î±'")
        df['Code']      = df['Code'].astype(str).str.strip()
        df['Î•Ï„Î±Î¹ÏÎµÎ¯Î±']  = df['Î•Ï„Î±Î¹ÏÎµÎ¯Î±'].astype(str).str.strip()
        return df[['Code','Î•Ï„Î±Î¹ÏÎµÎ¯Î±']]
    except Exception as e:
        log(fn, f"ERROR {e}")
        return None

def get_latest_green_ve6_files(folder: Path):
    fn = "get_latest_green_ve6_files"
    date_to_files = defaultdict(list)
    csvs = [f for f in os.listdir(folder) if f.startswith("GREEN_VE6") and f.endswith(".csv")]
    log(fn, f"Î’ÏÎ­Î¸Î·ÎºÎ±Î½ {len(csvs)} Î±ÏÏ‡ÎµÎ¯Î± ÏƒÏ„Î¿ {folder}")
    for name in csvs:
        m = re.match(r"GREEN_VE6(\d{8})(\d)\.csv", name)
        if not m:
            continue
        date    = m.group(1)
        edition = int(m.group(2))
        date_to_files[date].append((edition, name))
    latest = []
    for date in sorted(date_to_files.keys()):
        files = sorted(date_to_files[date], reverse=True)
        latest.append(files[0][1])
        log(fn, f"{date} -> Î­ÎºÎ´Î¿ÏƒÎ· {files[0][0]}: {files[0][1]}")
    return latest

def preprocess_timestamp_column(df):
    is_24 = df['TIMESTAMP'].astype(str).str.contains('24:00', regex=False)
    new_ts = df['TIMESTAMP'].astype(str)
    new_ts[is_24] = (
        pd.to_datetime(
            new_ts[is_24].str.replace('24:00','00:00'),
            format='%d/%m/%Y %H:%M',
            errors='coerce'
        ) + pd.Timedelta(days=1)
    ).dt.strftime('%d/%m/%Y %H:%M')
    df['TIMESTAMP'] = new_ts
    df['datetime']  = pd.to_datetime(df['TIMESTAMP'], format='%d/%m/%Y %H:%M', errors='coerce')
    return df

def safe_company_folder_name(name):
    return re.sub(r'[\\/*?:"<>|]', "", name.replace(" ", "_"))

def merge_with_existing_csv(group_df, out_file: Path):
    g = group_df.copy()
    g.set_index('TIMESTAMP', inplace=True)
    if out_file.exists():
        try:
            exist = pd.read_csv(out_file, delimiter=';', encoding='utf-8-sig')
            exist.set_index('TIMESTAMP', inplace=True)
            combined = exist[~exist.index.isin(g.index)]
            combined = pd.concat([combined, g])
        except Exception as e:
            log("merge_with_existing_csv", f"Î£Ï†Î¬Î»Î¼Î± Î±Î½Î¬Î³Î½Ï‰ÏƒÎ·Ï‚ {out_file}: {e}")
            combined = g
    else:
        combined = g
    combined = combined.reset_index()
    combined['datetime'] = pd.to_datetime(
        combined['TIMESTAMP'],
        format='%d/%m/%Y %H:%M',
        errors='coerce'
    )
    combined = combined.sort_values('datetime').drop(columns=['datetime'])
    return combined

def process_green_ve6_file(filepath: Path, producers_map: pd.DataFrame, output_folder: Path):
    fn = "process_green_ve6_file"
    log(fn, f"Î•Ï€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î±: {filepath.name}")
    try:
        df = pd.read_csv(filepath, delimiter=';', encoding='utf-8-sig', skiprows=1)
    except Exception as e:
        log(fn, f"Î£Ï†Î¬Î»Î¼Î± Î±Î½Î¬Î³Î½Ï‰ÏƒÎ·Ï‚: {e}")
        return
    if 'ÎšÎ©Î”Î™ÎšÎŸÎ£ Î•Î”Î¡Î•Î˜' not in df.columns or 'TIMESTAMP' not in df.columns:
        log(fn, "Î›ÎµÎ¯Ï€ÎµÎ¹ 'ÎšÎ©Î”Î™ÎšÎŸÎ£ Î•Î”Î¡Î•Î˜' Î® 'TIMESTAMP'")
        return
    df = preprocess_timestamp_column(df)
    for code_value, group in df.groupby('ÎšÎ©Î”Î™ÎšÎŸÎ£ Î•Î”Î¡Î•Î˜'):
        code_str = str(code_value).strip()
        row = producers_map[producers_map['Code'] == code_str]
        if row.empty:
            log(fn, f"Î†Î³Î½Ï‰ÏƒÏ„Î¿Ï‚ Code={code_str} (Î´ÎµÎ½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ ÏƒÏ„Î¿ producers.xlsx)")
            continue
        company   = row['Î•Ï„Î±Î¹ÏÎµÎ¯Î±'].values[0]
        safe_name = safe_company_folder_name(company)
        out_file  = output_folder / f"Î Î‘Î¡Î‘Î“Î©Î“Î—_{safe_name}.csv"
        final_df  = merge_with_existing_csv(group, out_file)
        final_df.to_csv(out_file, index=False, sep=';', encoding='utf-8-sig')
        log(fn, f"OK -> {out_file}")

def _month_iter(start_date: pd.Timestamp, end_date: pd.Timestamp):
    cur   = pd.Timestamp(start_date.year, start_date.month, 1)
    final = pd.Timestamp(end_date.year,   end_date.month,   1)
    while cur <= final:
        yield cur.year, cur.month
        cur = (cur + pd.offsets.MonthBegin(1))

def ensure_production_files(start_date: pd.Timestamp, end_date: pd.Timestamp):
    """
    Î§Ï„Î¯Î¶ÎµÎ¹/ÎµÎ½Î·Î¼ÎµÏÏÎ½ÎµÎ¹ Î Î‘Î¡Î‘Î“Î©Î“Î—_*.csv Î±Ï€ÏŒ downloads/<YYYY-MM>/GREEN_VE6*,
    Î³Î¹Î± ÏŒÎ»Î¿Ï…Ï‚ Ï„Î¿Ï…Ï‚ Î¼Î®Î½ÎµÏ‚ Ï€Î¿Ï… ÎºÎ±Î»ÏÏ€Ï„Î¿Ï…Î½ Ï„Î¿ [start_date .. end_date].
    """
    fn = "ensure_production_files_multi"
    producers_map = load_producers_basic(PRODUCERS_XLSX)
    if producers_map is None or producers_map.empty:
        print("âš ï¸ Î”ÎµÎ½ Î¼Ï€ÏŒÏÎµÏƒÎ± Î½Î± Î´Î¹Î±Î²Î¬ÏƒÏ‰ producers.xlsx (Î® Î»ÎµÎ¯Ï€Î¿Ï…Î½ Code/Î•Ï„Î±Î¹ÏÎµÎ¯Î±).")
        return

    PROD_DIR.mkdir(parents=True, exist_ok=True)
    total_sources = 0

    for y, m in _month_iter(start_date, end_date):
        month_tag    = f"{y}-{m:02d}"
        input_folder = DOWNLOADS_DIR / month_tag
        if not input_folder.is_dir():
            log(fn, f"skip: no downloads/{month_tag}")
            continue
        latest_files = get_latest_green_ve6_files(input_folder)
        if not latest_files:
            log(fn, f"skip: no GREEN_VE6 in {input_folder}")
            continue

        print(f"ğŸ”§ Î§Ï„Î¯Î¶Ï‰ Î Î‘Î¡Î‘Î“Î©Î“Î— Î±Ï€ÏŒ {len(latest_files)} GREEN_VE6 Î±ÏÏ‡ÎµÎ¯Î± Î³Î¹Î± {month_tag}...")
        for name in latest_files:
            process_green_ve6_file(input_folder / name, producers_map, PROD_DIR)
            total_sources += 1

    if total_sources:
        print("âœ… ÎˆÏ„Î¿Î¹Î¼Î¿ Ï„Î¿ Î Î‘Î¡Î‘Î“Î©Î“Î—/ (ÎµÎ½Î·Î¼ÎµÏÏÎ¸Î·ÎºÎµ Î³Î¹Î± ÏŒÎ»Î¿Ï…Ï‚ Ï„Î¿Ï…Ï‚ Î¼Î®Î½ÎµÏ‚)")
    else:
        print("â„¹ï¸ Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ Î½Î­Î± GREEN_VE6. Î£Ï…Î½ÎµÏ‡Î¯Î¶Ï‰ Î¼Îµ Ï„Î± Ï…Ï€Î¬ÏÏ‡Î¿Î½Ï„Î± Î Î‘Î¡Î‘Î“Î©Î“Î—_*.csv.")


# =================== DAM utilities (Î±Î½Ï„Î¹Î³ÏÎ±Ï†Î® Î±Ï€ÏŒ Î¼Î·Î½Î¹Î±Î¯Î¿) ===================

HEADER_TS_KEYS    = ["date", "time", "timestamp", "cet", "ce(s)t", "gmt", "utc", "eet", "athens", "gmt+2"]
HEADER_PRICE_KEYS = ["price", "eur/mwh", "â‚¬/mwh", "auction", "day-ahead", "day ahead"]

def _find_header_line(path, max_scan=200):
    with open(path, "r", encoding="utf-8-sig", errors="replace") as f:
        for i in range(max_scan):
            line = f.readline()
            if not line:
                break
            low = line.strip().lower()
            if any(k in low for k in HEADER_TS_KEYS) and any(k in low for k in HEADER_PRICE_KEYS):
                return i
    return 1

def _infer_dam_columns(df: pd.DataFrame):
    cols = list(df.columns)
    lower = {c: c.lower() for c in cols}

    ts_cands = [c for c in cols if any(k in lower[c] for k in HEADER_TS_KEYS)]
    price_cands = [c for c in cols if any(k in lower[c] for k in HEADER_PRICE_KEYS)]

    ts_col = ts_cands[0] if ts_cands else None
    price_cands = [c for c in price_cands if c != ts_col]
    price_col = price_cands[0] if price_cands else None

    if not ts_col:
        best, best_rate = None, -1
        for c in cols:
            try:
                parsed = pd.to_datetime(df[c], errors="coerce")
                rate = parsed.notna().sum() / max(1, df[c].notna().sum())
                if rate >= 0.8 and rate > best_rate:
                    best, best_rate = c, rate
            except Exception:
                pass
        ts_col = best
    if not price_col:
        best, best_rate = None, -1
        for c in cols:
            if c == ts_col:
                continue
            s = pd.to_numeric(df[c].astype(str).str.replace(",", ".", regex=False), errors="coerce")
            rate = s.notna().sum() / max(1, df[c].notna().sum())
            if rate >= 0.8 and rate > best_rate:
                best, best_rate = c, rate
        price_col = best

    if not ts_col or not price_col:
        raise ValueError(f"Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ ÏƒÏ„Î®Î»ÎµÏ‚ Timestamp/Price ÏƒÏ„Î¿ DAM CSV. Columns: {list(df.columns)}")
    return ts_col, price_col

def load_dam_quarterly_endtime(dam_csv_path: str, month: str):
    """
    Î”Î¹Î±Î²Î¬Î¶ÎµÎ¹ Ï„Î¿ Energy Charts CSV, Î²ÏÎ¯ÏƒÎºÎµÎ¹ header, Î¸ÎµÏ‰ÏÎµÎ¯ ÏŒÏ„Î¹ Ï„Î¿ timestamp ÎµÎ¯Î½Î±Î¹
    Î—Î”Î— local START time Î±Î½Î¬ 15Î»ÎµÏ€Ï„Î¿ (00:00, 00:15, ..., 23:45) ÎºÎ±Î¹
    Î”Î•Î Ï„Î¿ Î¼ÎµÏ„Î±ÎºÎ¹Î½ÎµÎ¯ -15'.

    Î•Ï€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹: TIMESTAMP (local START), DAM Price (â‚¬/MWh), dup_idx
    Î¼ÏŒÎ½Î¿ Î³Î¹Î± Ï„Î¿Î½ Î¶Î·Ï„Î¿ÏÎ¼ÎµÎ½Î¿ Î¼Î®Î½Î± (YYYY-MM), ÎºÎ±Î¹ Î¼ÏŒÎ½Î¿ Î±Ï€ÏŒ 2025-10-01 ÎºÎ±Î¹ Î¼ÎµÏ„Î¬.
    """
    fn = "load_dam_prices_15min"
    try:
        header_line = _find_header_line(dam_csv_path)
        dam = pd.read_csv(dam_csv_path, sep=None, engine="python", encoding="utf-8-sig", header=header_line)
        dam = dam.loc[:, ~dam.columns.astype(str).str.fullmatch(r"Unnamed: \d+")]
        dam.columns = [str(c).strip() for c in dam.columns]

        ts_col, price_col = _infer_dam_columns(dam)

        ts_aware = pd.to_datetime(dam[ts_col], errors="coerce", utc=True)
        if ts_aware.isna().all():
            start_local = pd.to_datetime(dam[ts_col], errors="coerce")  # naive local
        else:
            start_local = ts_aware.dt.tz_convert("Europe/Athens").dt.tz_localize(None)

        price = pd.to_numeric(
            dam[price_col].astype(str).str.replace(",", ".", regex=False),
            errors="coerce"
        )

        out = pd.DataFrame({
            "TIMESTAMP": start_local,
            "DAM Price (â‚¬/MWh)": price
        }).dropna(subset=["TIMESTAMP"])

        lb = pd.Timestamp("2025-10-01 00:00")
        out = out[out["TIMESTAMP"] >= lb]
        out = out[out["TIMESTAMP"].dt.strftime("%Y-%m") == month].copy()

        # Î”Î•Î ÎºÎ¬Î½Î¿Ï…Î¼Îµ sort: ÎºÏÎ±Ï„Î¬Î¼Îµ Ï„Î· ÏƒÎµÎ¹ÏÎ¬ Î±ÏÏ‡ÎµÎ¯Î¿Ï…, Î±Î»Î»Î¬ Î²Î¬Î¶Î¿Ï…Î¼Îµ dup_idx
        out["dup_idx"] = out.groupby("TIMESTAMP").cumcount()

        log(fn, f"DAM 15' rows after filters: {len(out)} for {month}")
        return out.reset_index(drop=True)

    except Exception as e:
        log(fn, f"ERROR {e}")
        return None


# =================== Production reading (Î¼Î®Î½Î¹Î±Î¯Î± Î»Î¿Î³Î¹ÎºÎ®) ===================

def read_production_data(file_path):
    fn = "read_production_data"
    try:
        df = pd.read_csv(file_path, sep=None, engine="python", encoding='utf-8-sig')
        log(fn, f"read {file_path}: {len(df)} rows")
        return df
    except Exception as e:
        log(fn, f"ERROR {file_path}: {e}")
        return None


# =================== ÎœÎ—ÎÎ™Î‘Î™ÎŸÎ£ Î¥Î ÎŸÎ›ÎŸÎ“Î™Î£ÎœÎŸÎ£ (copied) ===================

def calculate_daily_summary_quarterly(df_prod, df_dam_15m, producer_row, month):
    """
    Î‘ÎšÎ¡Î™Î’Î©Î£ Î· Î¯Î´Î¹Î± Î»Î¿Î³Î¹ÎºÎ® Î¼Îµ Ï„Î¿ Î¼Î·Î½Î¹Î±Î¯Î¿ ADMIE_MERGED_TIMOLOGIA:

      - Prod(END): D 00:15..23:45 + (D+1) 00:00
      - DAM(START): D 00:00..23:45
      - P[i] â†” DAM[i] Ï‡Ï‰ÏÎ¯Ï‚ shift.

    1/10: Ï€ÎµÏ„Î¬Î¼Îµ Ï„Î¹Ï‚ 00:15/00:30/00:45
    26/10: ÎºÏÎ±Ï„Î¬Î¼Îµ Î¼ÏŒÎ½Î¿ Ï„Î·Î½ Ï€ÏÏÏ„Î· ÎµÎ¼Ï†Î¬Î½Î¹ÏƒÎ· ÏƒÏ„Î± Î´Î¹Ï€Î»Î¬ 03:00â€“04:00 (prod & DAM).
    """
    fn = "calculate_daily_summary_15m_by_index"
    try:
        month_str = month

        # =============== 1. Î Î‘Î¡Î‘Î“Î©Î“Î— (END TS) ================
        prod = df_prod.copy()
        prod['END_TS'] = pd.to_datetime(
            prod['TIMESTAMP'],
            format="%d/%m/%Y %H:%M",
            errors='coerce',
            dayfirst=True
        )
        prod = prod.dropna(subset=['END_TS'])
        prod = prod[prod['END_TS'].dt.strftime("%Y-%m") == month_str].copy()
        if prod.empty:
            log(fn, f"no production rows for {month_str}")
            return None, None

        prod['Î•ÎÎ•Î¡Î“Î•Î™Î‘ (kWh)'] = pd.to_numeric(
            prod['Î•ÎÎ•Î¡Î“Î•Î™Î‘ (kWh)'], errors='coerce'
        ).fillna(0.0)
        prod = prod.sort_values('END_TS').reset_index(drop=True)

        # =============== 2. DAM (START TS) ================
        dam = df_dam_15m.copy()
        dam['START_TS'] = pd.to_datetime(dam['TIMESTAMP'], errors='coerce')
        dam = dam.dropna(subset=['START_TS'])
        dam = dam[dam['START_TS'].dt.strftime("%Y-%m") == month_str].copy()
        if dam.empty:
            log(fn, f"no DAM rows for {month_str}")
            return None, None

        dam = dam.sort_values('START_TS').reset_index(drop=True)

        # =============== 3. Î›Î¯ÏƒÏ„Î± Î·Î¼ÎµÏÏÎ½ ================
        prod['day'] = prod['END_TS'].dt.date
        days = sorted({d for d in prod['day'] if str(d).startswith(month_str)})
        if not days:
            log(fn, f"no days in production for {month_str}")
            return None, None

        all_quarters = []

        for D in days:
            D_ts = pd.Timestamp(str(D))
            D_next = D_ts + pd.Timedelta(days=1)

            # ---- Prod: (D,00:00]..(D+1,00:00] => 00:15..23:45 + next 00:00 ----
            day_prod = prod[
                (prod['END_TS'] > D_ts) & (prod['END_TS'] <= D_next)
            ].copy()

            # 01/10/2025: Ï€ÎµÏ„Î¬Î¼Îµ END 00:15/00:30/00:45
            if D == pd.Timestamp("2025-10-01").date():
                mask_skip = (
                    (day_prod['END_TS'].dt.date == D) & 
                    (day_prod['END_TS'].dt.hour == 0) &
                    (day_prod['END_TS'].dt.minute.isin([15, 30, 45]))
                )
                day_prod = day_prod[~mask_skip].copy()

            # 26/10/2025: intervals 03:00â€“04:00 â†’ END 03:15,03:30,03:45,04:00 (ÎºÏÎ±Ï„Î¬Î¼Îµ Ï€ÏÏÏ„Î·)
            if D == pd.Timestamp("2025-10-26").date():
                mask_win = (
                    ((day_prod['END_TS'].dt.hour == 3) & day_prod['END_TS'].dt.minute.isin([15, 30, 45])) |
                    ((day_prod['END_TS'].dt.hour == 4) & (day_prod['END_TS'].dt.minute == 0))
                )
                dup = day_prod[mask_win].duplicated(subset=['END_TS'], keep='first')
                day_prod = day_prod.drop(index=day_prod[mask_win].loc[dup].index)

            day_prod = day_prod.sort_values('END_TS').reset_index(drop=True)
            if day_prod.empty:
                continue

            # ---- DAM: D 00:00..23:45 ----
            day_dam = dam[
                (dam['START_TS'] >= D_ts) &
                (dam['START_TS'] <= D_ts + pd.Timedelta(hours=23, minutes=45))
            ].copy()

            # 26/10/2025 DAM: Î´Î¹Ï€Î»Î¬ 03:00â€“03:45 â†’ ÎºÏÎ±Ï„Î¬Î¼Îµ Ï€ÏÏÏ„Î· ÎµÎ¼Ï†Î¬Î½Î¹ÏƒÎ·
            if D == pd.Timestamp("2025-10-26").date():
                mask_dam_win = (
                    (day_dam['START_TS'].dt.hour == 3) &
                    (day_dam['START_TS'].dt.minute.isin([0, 15, 30, 45]))
                )
                dup_dam = day_dam[mask_dam_win].duplicated(subset=['START_TS'], keep='first')
                day_dam = day_dam.drop(index=day_dam[mask_dam_win].loc[dup_dam].index)

            day_dam = day_dam.sort_values('START_TS').reset_index(drop=True)
            if day_dam.empty:
                continue

            # ---- P[i] â†” DAM[i] ----
            n_p = len(day_prod)
            n_d = len(day_dam)
            n = min(n_p, n_d)
            if n == 0:
                continue
            if n_p != n_d:
                log(fn, f"Length mismatch {D}: prod={n_p}, dam={n_d}, using first {n}")

            day_prod = day_prod.iloc[:n].copy()
            day_dam = day_dam.iloc[:n].copy()

            kwh = day_prod['Î•ÎÎ•Î¡Î“Î•Î™Î‘ (kWh)'].to_numpy()
            price = day_dam['DAM Price (â‚¬/MWh)'].to_numpy()
            value_eur = (kwh * price) / 1000.0

            per_quarter = pd.DataFrame({
                'Î ÎµÏÎ¯Î¿Î´Î¿Ï‚ ÎµÎºÎºÎ±Î¸Î¬ÏÎ¹ÏƒÎ·Ï‚': [D] * n,
                'Î•ÎÎ•Î¡Î“Î•Î™Î‘ (kWh)': kwh,
                'Î‘Î¾Î¯Î± ÎµÎ½Î­ÏÎ³ÎµÎ¹Î±Ï‚ Î²Î¬ÏƒÎµÎ¹ Î¼ÎµÏ„ÏÎ®ÏƒÎµÏ‰Î½ (â‚¬)': value_eur
            })
            all_quarters.append(per_quarter)

        if not all_quarters:
            log(fn, f"no quarter rows after pairing for {month_str}")
            return None, None

        df_all = pd.concat(all_quarters, ignore_index=True)

        # =============== 4. Î—ÎœÎ•Î¡Î—Î£Î™Î‘ Î£Î¥ÎÎŸÎ›Î‘ ================
        df_daily = df_all.groupby('Î ÎµÏÎ¯Î¿Î´Î¿Ï‚ ÎµÎºÎºÎ±Î¸Î¬ÏÎ¹ÏƒÎ·Ï‚', as_index=False).agg({
            'Î•ÎÎ•Î¡Î“Î•Î™Î‘ (kWh)': 'sum',
            'Î‘Î¾Î¯Î± ÎµÎ½Î­ÏÎ³ÎµÎ¹Î±Ï‚ Î²Î¬ÏƒÎµÎ¹ Î¼ÎµÏ„ÏÎ®ÏƒÎµÏ‰Î½ (â‚¬)': 'sum'
        })

        df_daily['Î ÎµÏÎ¯Î¿Î´Î¿Ï‚ ÎµÎºÎºÎ±Î¸Î¬ÏÎ¹ÏƒÎ·Ï‚'] = pd.to_datetime(
            df_daily['Î ÎµÏÎ¯Î¿Î´Î¿Ï‚ ÎµÎºÎºÎ±Î¸Î¬ÏÎ¹ÏƒÎ·Ï‚']
        ).dt.strftime('%d/%m/%y')

        rate = float(producer_row['ÎœÎ¿Î½Î±Î´Î¹Î±Î¯Î± Î§ÏÎ­Ï‰ÏƒÎ· Î¦Î¿Î£Î•'].values[0])
        df_daily['Î ÏÎ¿Î¼Î®Î¸ÎµÎ¹Î± GREEN VALUE (â‚¬)'] = (
            df_daily['Î•ÎÎ•Î¡Î“Î•Î™Î‘ (kWh)'] / 1000.0 * rate
        ).round(2)

        df_daily['ÎœÎµÏƒÎ¿ÏƒÏ„Î±Î¸Î¼Î¹ÎºÎ® Î¤Î¹Î¼Î® Î‘Î³Î¿ÏÎ¬Ï‚ ÎºÎ±Ï„Î¬ Ï„Î¹Ï‚ ÏÏÎµÏ‚ Ï€Î±ÏÎ±Î³Ï‰Î³Î®Ï‚ Ï„Î¿Ï… ÏƒÏ„Î±Î¸Î¼Î¿Ï'] = df_daily.apply(
            lambda row: 0 if row['Î•ÎÎ•Î¡Î“Î•Î™Î‘ (kWh)'] == 0 else round(
                (row['Î‘Î¾Î¯Î± ÎµÎ½Î­ÏÎ³ÎµÎ¹Î±Ï‚ Î²Î¬ÏƒÎµÎ¹ Î¼ÎµÏ„ÏÎ®ÏƒÎµÏ‰Î½ (â‚¬)'] / row['Î•ÎÎ•Î¡Î“Î•Î™Î‘ (kWh)']) * 1000.0, 2
            ),
            axis=1
        )

        sum_energy = round(float(df_daily['Î•ÎÎ•Î¡Î“Î•Î™Î‘ (kWh)'].sum()), 2)
        sum_value  = round(float(df_daily['Î‘Î¾Î¯Î± ÎµÎ½Î­ÏÎ³ÎµÎ¹Î±Ï‚ Î²Î¬ÏƒÎµÎ¹ Î¼ÎµÏ„ÏÎ®ÏƒÎµÏ‰Î½ (â‚¬)'].sum()), 2)
        sum_prov   = round(float(df_daily['Î ÏÎ¿Î¼Î®Î¸ÎµÎ¹Î± GREEN VALUE (â‚¬)'].sum()), 2)

        summary_row = pd.DataFrame([{
            'Î ÎµÏÎ¯Î¿Î´Î¿Ï‚ ÎµÎºÎºÎ±Î¸Î¬ÏÎ¹ÏƒÎ·Ï‚': 'Î£ÏÎ½Î¿Î»Î¿',
            'Î•ÎÎ•Î¡Î“Î•Î™Î‘ (kWh)': sum_energy,
            'Î‘Î¾Î¯Î± ÎµÎ½Î­ÏÎ³ÎµÎ¹Î±Ï‚ Î²Î¬ÏƒÎµÎ¹ Î¼ÎµÏ„ÏÎ®ÏƒÎµÏ‰Î½ (â‚¬)': sum_value,
            'Î ÏÎ¿Î¼Î®Î¸ÎµÎ¹Î± GREEN VALUE (â‚¬)': sum_prov,
            'ÎœÎµÏƒÎ¿ÏƒÏ„Î±Î¸Î¼Î¹ÎºÎ® Î¤Î¹Î¼Î® Î‘Î³Î¿ÏÎ¬Ï‚ ÎºÎ±Ï„Î¬ Ï„Î¹Ï‚ ÏÏÎµÏ‚ Ï€Î±ÏÎ±Î³Ï‰Î³Î®Ï‚ Ï„Î¿Ï… ÏƒÏ„Î±Î¸Î¼Î¿Ï': (
                round((sum_value / sum_energy) * 1000.0, 2) if sum_energy > 0 else 0
            )
        }])

        df_final = pd.concat([df_daily, summary_row], ignore_index=True)
        log(fn, f"daily rows={len(df_final)} for month={month_str}")
        return df_final, (sum_energy, sum_value, sum_prov)

    except Exception as e:
        log(fn, f"ERROR {e}")
        return None, None


# =================== WEEKLY SUMMARY (filter Ï€Î¬Î½Ï‰ ÏƒÏ„Î¿ Î¼Î·Î½Î¹Î±Î¯Î¿) ===================

def calculate_weekly_summary_from_month(df_prod, df_dam_month, producer_row, month_str, start_date, end_date):
    """
    1) Î¤ÏÎ­Ï‡ÎµÎ¹ calculate_daily_summary_quarterly Î³Î¹Î± ÎŸÎ›ÎŸ Ï„Î¿Î½ Î¼Î®Î½Î±.
    2) Î ÎµÏ„Î¬ÎµÎ¹ Ï„Î· Î³ÏÎ±Î¼Î¼Î® 'Î£ÏÎ½Î¿Î»Î¿'.
    3) ÎšÏÎ±Ï„Î¬ Î¼ÏŒÎ½Î¿ Ï„Î¹Ï‚ Î¼Î­ÏÎµÏ‚ [start_date .. end_date].
    4) ÎÎ±Î½Î±Ï‹Ï€Î¿Î»Î¿Î³Î¯Î¶ÎµÎ¹ Ï„Î± ÏƒÏÎ½Î¿Î»Î± Î³Î¹Î± Î±Ï…Ï„Î­Ï‚ Ï„Î¹Ï‚ Î¼Î­ÏÎµÏ‚.
    """
    fn = "calculate_weekly_summary_from_month"
    try:
        df_month, summary_month = calculate_daily_summary_quarterly(
            df_prod, df_dam_month, producer_row, month_str
        )
        if df_month is None or df_month.empty:
            log(fn, f"no monthly summary for {month_str}")
            return None, None

        df_no_total = df_month[df_month['Î ÎµÏÎ¯Î¿Î´Î¿Ï‚ ÎµÎºÎºÎ±Î¸Î¬ÏÎ¹ÏƒÎ·Ï‚'] != 'Î£ÏÎ½Î¿Î»Î¿'].copy()

        df_no_total['date_obj'] = pd.to_datetime(
            df_no_total['Î ÎµÏÎ¯Î¿Î´Î¿Ï‚ ÎµÎºÎºÎ±Î¸Î¬ÏÎ¹ÏƒÎ·Ï‚'],
            format='%d/%m/%y',
            errors='coerce'
        )
        mask = (df_no_total['date_obj'] >= start_date) & (df_no_total['date_obj'] <= end_date)
        df_week = df_no_total[mask].copy()
        if df_week.empty:
            log(fn, f"no rows in requested week {start_date}..{end_date}")
            return None, None

        sum_energy = round(float(df_week['Î•ÎÎ•Î¡Î“Î•Î™Î‘ (kWh)'].sum()), 2)
        sum_value  = round(float(df_week['Î‘Î¾Î¯Î± ÎµÎ½Î­ÏÎ³ÎµÎ¹Î±Ï‚ Î²Î¬ÏƒÎµÎ¹ Î¼ÎµÏ„ÏÎ®ÏƒÎµÏ‰Î½ (â‚¬)'].sum()), 2)
        sum_prov   = round(float(df_week['Î ÏÎ¿Î¼Î®Î¸ÎµÎ¹Î± GREEN VALUE (â‚¬)'].sum()), 2)

        summary_row = pd.DataFrame([{
            'Î ÎµÏÎ¯Î¿Î´Î¿Ï‚ ÎµÎºÎºÎ±Î¸Î¬ÏÎ¹ÏƒÎ·Ï‚': 'Î£ÏÎ½Î¿Î»Î¿ Î•Î²Î´Î¿Î¼Î¬Î´Î±Ï‚',
            'Î•ÎÎ•Î¡Î“Î•Î™Î‘ (kWh)': sum_energy,
            'Î‘Î¾Î¯Î± ÎµÎ½Î­ÏÎ³ÎµÎ¹Î±Ï‚ Î²Î¬ÏƒÎµÎ¹ Î¼ÎµÏ„ÏÎ®ÏƒÎµÏ‰Î½ (â‚¬)': sum_value,
            'Î ÏÎ¿Î¼Î®Î¸ÎµÎ¹Î± GREEN VALUE (â‚¬)': sum_prov,
            'ÎœÎµÏƒÎ¿ÏƒÏ„Î±Î¸Î¼Î¹ÎºÎ® Î¤Î¹Î¼Î® Î‘Î³Î¿ÏÎ¬Ï‚ ÎºÎ±Ï„Î¬ Ï„Î¹Ï‚ ÏÏÎµÏ‚ Ï€Î±ÏÎ±Î³Ï‰Î³Î®Ï‚ Ï„Î¿Ï… ÏƒÏ„Î±Î¸Î¼Î¿Ï': (
                round((sum_value / sum_energy) * 1000.0, 2) if sum_energy > 0 else 0
            )
        }])

        df_week = df_week.drop(columns=['date_obj'])
        df_final = pd.concat([df_week, summary_row], ignore_index=True)
        log(fn, f"weekly rows={len(df_final)}")
        return df_final, (sum_energy, sum_value, sum_prov)

    except Exception as e:
        log(fn, f"ERROR {e}")
        return None, None


# =================== Excel + PDF (weekly) ===================

def _rect_bounds(coord: str):
    if ":" not in coord:
        c, r = coordinate_to_tuple(coord)
        return (c, r, c, r)
    a, b = coord.split(":")
    c1, r1 = coordinate_to_tuple(a)
    c2, r2 = coordinate_to_tuple(b)
    return (min(c1, c2), min(r1, r2), max(c1, c2), max(r1, r2))

def _ranges_overlap(a_bounds, b_bounds):
    aL,aT,aR,aB = a_bounds
    bL,bT,bR,bB = b_bounds
    return not (aR < bL or bR < aL or aB < bT or bB < aT)

def _unmerge_in_rect(ws: Worksheet, rect: str):
    target = _rect_bounds(rect)
    to_unmerge = []
    for rg in list(ws.merged_cells.ranges):
        if _ranges_overlap(target, rg.bounds):
            to_unmerge.append(rg.coord)
    for coord in to_unmerge:
        try:
            ws.unmerge_cells(coord)
        except Exception as e:
            log("_unmerge_in_rect", f"Failed to unmerge {coord}: {e}")

def set_cell_value(ws: Worksheet, coord: str, value):
    """
    Safely set a cell value, handling merged cells by unmerging first.
    """
    # Try to unmerge any range containing this cell
    merged_ranges_to_unmerge = []
    for mrange in list(ws.merged_cells.ranges):
        if coord in mrange:
            merged_ranges_to_unmerge.append(mrange.coord)
    
    for mrange_coord in merged_ranges_to_unmerge:
        try:
            ws.unmerge_cells(mrange_coord)
        except Exception as e:
            log("set_cell_value", f"Failed to unmerge {mrange_coord}: {e}")
    
    # Now try to set the value, retrying if still MergedCell
    max_attempts = 3
    for attempt in range(max_attempts):
        try:
            cell = ws[coord]
            if not isinstance(cell, MergedCell):
                cell.value = value
                return
            # If still merged, try harder to unmerge
            for mrange in list(ws.merged_cells.ranges):
                if coord in mrange:
                    ws.unmerge_cells(mrange.coord)
        except Exception as e:
            if attempt == max_attempts - 1:
                log("set_cell_value", f"Failed to set value at {coord} after {max_attempts} attempts: {e}")
            time.sleep(0.01)
    
    # Last resort: try to write to the cell directly
    try:
        ws[coord].value = value
    except Exception as e:
        log("set_cell_value", f"Final attempt failed for {coord}: {e}")

def set_cell_property(ws: Worksheet, coord: str, prop_name: str, prop_value):
    """
    Safely set a cell property (font, alignment, etc.), handling merged cells.
    For merged cells, accesses the top-left cell of the merge.
    """
    cell = ws[coord]
    if isinstance(cell, MergedCell):
        # Find the top-left cell of the merged range
        for mrange in ws.merged_cells.ranges:
            if coord in mrange:
                min_col, min_row = mrange.bounds[0], mrange.bounds[1]
                cell = ws.cell(row=min_row, column=min_col)
                break
    
    try:
        setattr(cell, prop_name, prop_value)
    except Exception as e:
        log("set_cell_property", f"Failed to set {prop_name} on {coord}: {e}")

def _safe_merge(ws, coord: str):
    try:
        try:
            ws.unmerge_cells(coord)
        except Exception:
            pass
        ws.merge_cells(coord)
        return True
    except Exception as e:
        log("generate_invoice_excel_weekly", f"merge {coord} failed: {e}")
        return False

def _add_logo_if_available(ws, base_dir: Path):
    try:
        existing = getattr(ws, "_images", [])
        if existing and len(existing) > 0:
            log("generate_invoice_excel_weekly", "Template already has a logo; skipping extra logo.")
            return
    except Exception:
        pass

    try:
        from openpyxl.drawing.image import Image as XLImage
    except Exception as e:
        log("generate_invoice_excel_weekly", f"Pillow/openpyxl image support missing: {e}")
        return

    logo_path = base_dir / "logo.png"
    if not logo_path.exists():
        log("generate_invoice_excel_weekly", f"logo not found: {logo_path}")
        return

    try:
        img = XLImage(str(logo_path))
        img.anchor = "A1"
        img.width  = 120
        img.height = 60
        ws.add_image(img)
        log("generate_invoice_excel_weekly", f"logo added (scaled) from {logo_path}")
    except Exception as e:
        log("generate_invoice_excel_weekly", f"logo add failed: {e}")

def make_week_dirs(start_date: pd.Timestamp, end_date: pd.Timestamp):
    iso_year, iso_week, _ = start_date.isocalendar()
    tag = f"{iso_year}-W{iso_week:02d}"
    root    = BASE_DIR / "Î•ÎÎ—ÎœÎ•Î¡Î©Î¤Î™ÎšÎ‘_Î£Î—ÎœÎ•Î™Î©ÎœÎ‘Î¤Î‘_Î•Î’Î”ÎŸÎœÎ‘Î”Î™Î‘Î™Î‘" / tag
    xlsx_dir = root / "XLSX"
    pdf_dir  = root / "PDF"
    xlsx_dir.mkdir(parents=True, exist_ok=True)
    pdf_dir.mkdir(parents=True, exist_ok=True)
    return tag, root, xlsx_dir, pdf_dir

def determine_pdf_subfolder_name(email, email_to_companies):
    companies = email_to_companies.get(email, [])
    return clipped_folder_name([], companies, limit=MAX_FOLDER_CHARS)

def generate_invoice_excel_weekly(df_daily, summary, producer_row, start_date, end_date, xlsx_output_dir: Path, tag: str):
    fn = "generate_invoice_excel_weekly"
    try:
        if not TEMPLATE_FILE.exists():
            raise FileNotFoundError(f"Î›ÎµÎ¯Ï€ÎµÎ¹ template: {TEMPLATE_FILE}")

        company_name = str(producer_row['Î•Ï„Î±Î¹ÏÎµÎ¯Î±'].values[0])
        email_value  = str(producer_row.get('Email', "")) if 'Email' in producer_row else ''
        iban         = producer_row['IBAN'].values[0] if 'IBAN' in producer_row else ''
        rate         = float(producer_row['ÎœÎ¿Î½Î±Î´Î¹Î±Î¯Î± Î§ÏÎ­Ï‰ÏƒÎ· Î¦Î¿Î£Î•'].values[0])
        sum_energy, sum_value, sum_prov = summary

        out_name  = xlsx_filename_weekly(company_name, tag)
        xlsx_path = xlsx_output_dir / out_name
        xlsx_output_dir.mkdir(parents=True, exist_ok=True)

        wb = load_workbook(TEMPLATE_FILE)
        ws = wb.active

        try:
            ws.print_area = 'A1:H55'
        except Exception:
            pass
        try:
            ws.page_setup.fitToWidth  = 1
            ws.page_setup.fitToHeight = 0
        except Exception:
            pass

        # âš ï¸ Î”ÎµÎ½ Ï€ÎµÎ¹ÏÎ¬Î¶Î¿Ï…Î¼Îµ heights Î³ÏÎ±Î¼Î¼ÏÎ½ â€” Î¼Î­Î½Î¿Ï…Î½ ÏŒÏ€Ï‰Ï‚ ÎµÎ¯Î½Î±Î¹ ÏƒÏ„Î¿ template

        _add_logo_if_available(ws, BASE_DIR)

        from openpyxl.styles import Font, Alignment

        # === Header: ÏŒÎ»Î± merged Î¼Î±Î¶Î¯ (D1:F2) ÎºÎ±Î¹ ÏŒÎ»Î± ÏƒÏ„Î¿ ÎºÎ­Î½Ï„ÏÎ¿ ===
        _safe_merge(ws, "D1:F2")
        ws["D1"].value = (
            "Î•Î½Î·Î¼ÎµÏÏ‰Ï„Î¹ÎºÏŒ Î£Î·Î¼ÎµÎ¯Ï‰Î¼Î± Î•Î²Î´Î¿Î¼Î¬Î´Î±Ï‚\n"
            f"{start_date.strftime('%d/%m/%y')} â€“ {end_date.strftime('%d/%m/%y')}"
        )
        ws["D1"].font = Font(bold=True, size=11)
        ws["D1"].alignment = Alignment(
            horizontal="center",
            vertical="center",
            wrap_text=True
        )

        # === Merge G1:H2 ÎºÎ±Î¹ ÎºÎµÎ½Ï„ÏÎ¬ÏÎ¹ÏƒÎ¼Î± ===
        _safe_merge(ws, "G1:H1")
        ws["G1"].value = (
            'Î¦Î¿ÏÎ­Î±Ï‚ Î£Ï‰ÏÎµÏ…Ï„Î¹ÎºÎ®Ï‚ Î•ÎºÏ€ÏÎ¿ÏƒÏÏ€Î·ÏƒÎ·Ï‚ Î‘Î Î• (Î¦Î¿.Î£.Î•.)\n'
            'Î”Î¹ÎµÏÎ¸Ï…Î½ÏƒÎ·: Î¦Î¹Î»Î¿Ï€Î¬Ï€Ï€Î¿Ï… 19, Î‘Î¸Î®Î½Î± 11741, Î•Î»Î»Î¬Î´Î±\n'
            'Î‘Î¦Îœ: 801961185\n'
            'Î“Î•ÎœÎ—: 167104201000\n' \
            'Î”ÎŸÎ¥:Î¦Î‘Î• Î‘Î¸Î·Î½ÏÎ½\n' \
            'Email: info@greenvalue.gr'
        )
        ws["G1"].font = Font(bold=True, size=10)
        ws["G1"].alignment = Alignment(
            horizontal="right",
            vertical="center",
            wrap_text=True
        )
        # set_cell_value(
        #     ws, "G1",
        #     'Î¦Î¿ÏÎ­Î±Ï‚ Î£Ï‰ÏÎµÏ…Ï„Î¹ÎºÎ®Ï‚ Î•ÎºÏ€ÏÎ¿ÏƒÏÏ€Î·ÏƒÎ·Ï‚ Î‘Î Î• (Î¦Î¿.Î£.Î•.)\n'
        #     'Î”Î¹ÎµÏÎ¸Ï…Î½ÏƒÎ·: Î¦Î¹Î»Î¿Ï€Î¬Ï€Ï€Î¿Ï… 19, Î‘Î¸Î®Î½Î± 11741, Î•Î»Î»Î¬Î´Î±\n'
        #     'Î‘Î¦Îœ: 801961185\n'
        #     'Î“Î•ÎœÎ—: 167104201000\n' \
        #     'Î”ÎŸÎ¥:Î¦Î‘Î• Î‘Î¸Î·Î½ÏÎ½\n' \
        #     'Email: info@greenvalue.gr'
        # )
        # set_cell_property(ws, "G1", "font", Font(size=10))
        # set_cell_property(
        #     ws, "G1", "alignment",
        #     Alignment(
        #         wrap_text=True,
        #         horizontal="right",
        #         vertical="center"
        #     )
        # )

        needed = ['Î‘.Îœ. Î‘Î Î•','Î•Ï„Î±Î¹ÏÎµÎ¯Î±','Î‘Î¦Îœ','Î”ÎŸÎ¥','Î”Î¹ÎµÏÎ¸Ï…Î½ÏƒÎ·','Email','Î¤ÎµÏ‡Î½Î¿Î»Î¿Î³Î¯Î±']
        vals   = producer_row.iloc[0][needed].tolist()

        # Place producer/park info one row lower (row 4)
        for cell_ref, val in zip(['B4','C4','D4','E4','F4','G4','H4'], vals):
            set_cell_value(ws, cell_ref, val)
        set_cell_property(ws, 'C4', 'font', Font(size=13))

        # === Merge D6:F6 ÎºÎ±Î¹ ÎºÎµÎ½Ï„ÏÎ¬ÏÎ¹ÏƒÎ¼Î± ===
        _safe_merge(ws, "D6:F6")
        ws["D6"].value = (
            f"{start_date.strftime('%d/%m/%y')} - {end_date.strftime('%d/%m/%y')}"
        )
        ws["D6"].font = Font(bold=True, size=14)
        ws["D6"].alignment = Alignment(
            horizontal="center",
            vertical="center",
            wrap_text=True
        )
        # set_cell_value(
        #     ws, "D6:F6",
        #     f"{start_date.strftime('%d/%m/%y')}-{end_date.strftime('%d/%m/%y')}"
        # )
        # set_cell_property(
        #     ws, "D6:F6", "alignment",
        #     Alignment(horizontal="center", vertical="center", wrap_text=True)
        # )

        # Unmerge ALL merged cells in the data area (C10:G41) to avoid MergedCell errors
        for rg in list(ws.merged_cells.ranges):
            bounds = rg.bounds
            # Check if this merge overlaps with data area C10:G41
            if (bounds[1] >= 10 and bounds[1] <= 19 and bounds[0] >= 3 and bounds[0] <= 7):
                try:
                    ws.unmerge_cells(rg.coord)
                    log(fn, f"Unmerged: {rg.coord}")
                except Exception as e:
                    log(fn, f"Failed to unmerge {rg.coord}: {e}")

        for row in ws.iter_rows(min_row=10, max_row=19, min_col=3, max_col=7):
            for cell in row:
                try:
                    cell.value = None
                except Exception as e:
                    log(fn, f"Could not clear {cell.coordinate}: {e}")

        start_row = 10
        for r_idx, row_vals in enumerate(df_daily.values, start=start_row):
            for c_idx, value in enumerate(row_vals, start=3):
                try:
                    cell = ws.cell(row=r_idx, column=c_idx, value=value)
                    if isinstance(value, (int, float)):
                        try:
                            cell.number_format = '#,##0.00'
                        except Exception:
                            pass
                except Exception as e:
                    log(fn, f"Could not set cell ({r_idx},{c_idx}): {e}")

        total_row = start_row + len(df_daily) - 1
        for col in range(3, 8):
            cell_coord = f"{chr(64 + col)}{total_row}"
            set_cell_property(ws, cell_coord, 'font', Font(bold=True, size=15))

        _safe_merge(ws, "C28:D28")
        # set_cell_value(ws, 'C28', iban)
        # set_cell_property(ws, 'C28', 'font', Font(size=14, bold=True))

        ws["C28"].value = iban
        ws["C28"].font = Font(bold=True, size=14)
        ws["C28"].alignment = Alignment(
            horizontal="right",
            vertical="center",
            wrap_text=True
        )

        

        _safe_merge(ws, "C29:D29")
        set_cell_value(ws, 'C29', (pd.Timestamp.today() + pd.Timedelta(days=5)).strftime('%d/%m/%y'))
        set_cell_value(ws, 'D21', rate)
        set_cell_value(ws, 'D22', round(sum_prov, 2))
        set_cell_property(ws, 'D43', 'number_format', '#,##0.00')

        wb.save(xlsx_path)
        log(fn, f"XLSX OK: {xlsx_path}")
        print(f"âœ… XLSX â†’ {xlsx_path}")
        return str(xlsx_path), company_name, email_value

    except Exception as e:
        log(fn, f"ERROR {e}")
        print(f"âŒ Excel generation failed: {e}")
        return None, None, None


    except Exception as e:
        log(fn, f"ERROR {e}")
        print(f"âŒ Excel generation failed: {e}")
        return None, None, None




# def _find_soffice_path() -> str | None:
#     p = shutil.which("soffice")
#     if p:
#         return p
#     default = "/Applications/LibreOffice.app/Contents/MacOS/soffice"
#     if os.path.exists(default):
#         return default
#     return None

def export_to_pdf_with_excel(xlsx_path: str, pdf_path: str) -> tuple[bool, str]:
    fn = "export_to_pdf_excel_weekly"
    if not _HAS_XLWINGS:
        log(fn, "xlwings not available; skip Excel export.")
        return False, "excel-not-available"
    import xlwings as xw  # type: ignore
    app = xw.App(visible=False, add_book=False)
    try:
        wb = app.books.open(os.path.abspath(xlsx_path))
        try:
            sht = wb.sheets.active
            sht.api.PageSetup.Zoom         = False
            sht.api.PageSetup.FitToPagesWide  = 1
            sht.api.PageSetup.FitToPagesTall  = False
        except Exception as e:
            log(fn, f"PageSetup warn: {e}")

        out_pdf = os.path.abspath(pdf_path)
        Path(os.path.dirname(out_pdf)).mkdir(parents=True, exist_ok=True)

        # 1) Try ExportAsFixedFormat (Windows-style COM API)
        # export_failed = False
        # try:
        #     wb.api.ExportAsFixedFormat(0, out_pdf)
        # except Exception as e_api:
        #     log(fn, f"ExportAsFixedFormat failed: {e_api}; trying fallback...")
        #     export_failed = True

        # 2) If ExportAsFixedFormat failed, try xlwings wb.to_pdf (if available)
        try:
                if hasattr(wb, "to_pdf"):
                    wb.to_pdf(out_pdf)
                    export_failed = False
                else:
                    raise AttributeError("wb.to_pdf not available")
        except Exception as e_to_pdf:
                log(fn, f"wb.to_pdf failed: {e_to_pdf}; trying AppleScript fallback...")

                # 3) AppleScript fallback: ask Microsoft Excel (Mac) to save as PDF
                # try:
                #     applescript = (
                #         'tell application "Microsoft Excel"\n'
                #         f'    open POSIX file "{os.path.abspath(xlsx_path)}"\n'
                #         '    delay 0.5\n'
                #         '    tell workbook 1\n'
                #         f'        save workbook as filename POSIX file "{out_pdf}" file format PDF file format\n'
                #         '        close saving no\n'
                #         '    end tell\n'
                #         'end tell'
                #     )
                #     res = subprocess.run(["osascript", "-e", applescript], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, timeout=60)
                #     if res.returncode == 0:
                #         export_failed = False
                #         log(fn, "AppleScript export succeeded")
                #     else:
                #         log(fn, f"AppleScript failed rc={res.returncode}\nstdout:{res.stdout}\nstderr:{res.stderr}")
                # except Exception as e_apple:
                #     log(fn, f"AppleScript fallback failed: {e_apple}")

        wb.close()

        ok = os.path.exists(out_pdf) and os.path.getsize(out_pdf) >= 500
        if ok:
            log(fn, f"OK via Excel -> {out_pdf}")
            print(f"âœ… PDF (Excel) â†’ {out_pdf}")
            return True, "excel"
        else:
            log(fn, f"Excel produced no/empty file at: {out_pdf}")
            print(f"âš ï¸ Empty PDF from Excel â†’ {out_pdf}")
            return False, "excel-empty"
    except Exception as e:
        log(fn, f"ERROR (Excel): {e}")
        print(f"âŒ PDF export (Excel) failed: {e}")
        return False, f"excel-error:{e}"
    finally:
        try:
            app.quit()
        except Exception:
            pass

# def export_to_pdf_with_libreoffice(xlsx_path: str, pdf_path: str) -> tuple[bool, str]:
#     fn = "export_to_pdf_libreoffice_weekly"
#     soffice = _find_soffice_path()
#     if not soffice:
#         log(fn, "LibreOffice 'soffice' not found.")
#         return False, "lo-missing"

#     outdir = os.path.abspath(os.path.dirname(pdf_path))
#     Path(outdir).mkdir(parents=True, exist_ok=True)

#     cmd = [
#         soffice,
#         "--headless","--norestore","--nolockcheck",
#         "--convert-to","pdf",
#         "--outdir", outdir,
#         os.path.abspath(xlsx_path)
#     ]

#     try:
#         res = subprocess.run(
#             cmd,
#             stdout=subprocess.PIPE,
#             stderr=subprocess.PIPE,
#             text=True,
#             timeout=180
#         )
#         if res.returncode != 0:
#             log(fn, f"ERROR rc={res.returncode}\nstdout:\n{res.stdout}\nstderr:\n{res.stderr}")
#             print(f"âŒ PDF export (LibreOffice) failed rc={res.returncode}")
#             return False, f"lo-error-rc{res.returncode}"

#         produced = os.path.join(outdir, os.path.splitext(os.path.basename(xlsx_path))[0] + ".pdf")
#         if os.path.abspath(produced) != os.path.abspath(pdf_path):
#             try:
#                 if os.path.exists(pdf_path):
#                     os.remove(pdf_path)
#                 os.replace(produced, pdf_path)
#             except Exception as e:
#                 log(fn, f"Rename error: {e}")
#                 return False, f"lo-rename-error:{e}"

#         time.sleep(0.2)
#         ok = os.path.exists(pdf_path) and os.path.getsize(pdf_path) >= 500
#         if ok:
#             log(fn, f"OK via LibreOffice -> {pdf_path}")
#             print(f"âœ… PDF (LibreOffice) â†’ {pdf_path}")
#             return True, "libreoffice"
#         else:
#             log(fn, f"LibreOffice produced no/empty file at: {pdf_path}")
#             print(f"âš ï¸ Empty PDF from LibreOffice â†’ {pdf_path}")
#             return False, "lo-empty"

#     except Exception as e:
#         log(fn, f"ERROR (LibreOffice): {e}")
#         print(f"âŒ PDF export (LibreOffice) exception: {e}")
#         return False, f"lo-exception:{e}"

def export_to_pdf(xlsx_path: str, pdf_path: str) -> tuple[bool, str]:
    ok, how = export_to_pdf_with_excel(xlsx_path, pdf_path)
    if ok:
        return True, how


# =================== Main weekly job ===================

# def timologia_weekly(start_date_str: str, end_date_str: str):
#     # Parse dates
#     try:
#         start_date = pd.to_datetime(start_date_str).floor('D')
#         end_date   = pd.to_datetime(end_date_str).floor('D')
#     except Exception:
#         print("ÎœÎ· Î­Î³ÎºÏ…ÏÎµÏ‚ Î·Î¼ÎµÏÎ¿Î¼Î·Î½Î¯ÎµÏ‚. Î§ÏÎ®ÏƒÎ·: YYYY-MM-DD")
#         return
#     if end_date < start_date:
#         print("Î¤Î¿ Ï„Î­Î»Î¿Ï‚ ÎµÎ¯Î½Î±Î¹ Ï€ÏÎ¹Î½ Ï„Î·Î½ Î±ÏÏ‡Î®.")
#         return

#     if start_date.year != end_date.year or start_date.month != end_date.month:
#         print("Î ÏÎ¿Ï‚ Ï„Î¿ Ï€Î±ÏÏŒÎ½ Î· ÎµÎ²Î´Î¿Î¼Î¬Î´Î± Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± ÎµÎ¯Î½Î±Î¹ Î¼Î­ÏƒÎ± ÏƒÏ„Î¿Î½ Î¯Î´Î¹Î¿ Î¼Î®Î½Î±.")
#         print("Î§ÏÏÎ¹ÏƒÎ­ Ï„Î·Î½ ÏƒÎµ Î´ÏÎ¿ ÎºÎ»Î®ÏƒÎµÎ¹Ï‚ (Î¼Î¯Î± Î³Î¹Î± ÎºÎ¬Î¸Îµ Î¼Î®Î½Î±).")
#         return

#     month_str = start_date.strftime('%Y-%m')

#     # 1) Î§Ï„Î¯Î¶Î¿Ï…Î¼Îµ/ÎµÎ½Î·Î¼ÎµÏÏÎ½Î¿Ï…Î¼Îµ Î Î‘Î¡Î‘Î“Î©Î“Î— Î±Ï€ÏŒ GREEN_VE6
#     # ensure_production_files(start_date, end_date)

#     # 2) Î¦Î¬ÎºÎµÎ»Î¿Î¹ ÎµÎ¾ÏŒÎ´Î¿Ï…
#     tag, root, xlsx_dir, pdf_dir = make_week_dirs(start_date, end_date)

#     # 3) Producers (Î£Î—Î˜Î¥Î‘ Î¼ÏŒÎ½Î¿)
#     producers_df = load_producers_sithya(PRODUCERS_XLSX)
#     print(producers_df)
#     if producers_df is None or producers_df.empty:
#         print("Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ Ï€Î±ÏÎ±Î³Ï‰Î³Î¿Î¯ Î£Î—Î˜Î¥Î‘.")
#         return
#     email_to_companies, _ = build_email_groups(producers_df)

#     # 4) DAM 15' Î³Î¹Î± Ï„Î¿Î½ Î¼Î®Î½Î± (START, 2025-10-01+)
#     if not DAM_FILE_2025.exists():
#         print(f"Î›ÎµÎ¯Ï€ÎµÎ¹ DAM CSV: {DAM_FILE_2025.name}")
#         return
#     df_dam_month = load_dam_quarterly_endtime(str(DAM_FILE_2025), month_str)
#     if df_dam_month is None or df_dam_month.empty:
#         print("Î‘Ï€Î¿Ï„Ï…Ï‡Î¯Î±: DAM 15' prices (empty).")
#         return

#     # 5) Î Î‘Î¡Î‘Î“Î©Î“Î—_*.csv
#     if not PROD_DIR.is_dir():
#         print(f"Î›ÎµÎ¯Ï€ÎµÎ¹ Ï†Î¬ÎºÎµÎ»Î¿Ï‚: {PROD_DIR} (ÎºÎ±Î¹ Î´ÎµÎ½ Î¼Ï€ÏŒÏÎµÏƒÎ± Î½Î± Ï„Î¿Î½ Ï†Ï„Î¹Î¬Î¾Ï‰ Î±Ï€ÏŒ downloads)")
#         return

#     for filename in os.listdir(PROD_DIR):
#         if not (filename.startswith('Î Î‘Î¡Î‘Î“Î©Î“Î—_') and filename.endswith('.csv')):
#             continue

#         file_path = PROD_DIR / filename
#         m = re.match(r'Î Î‘Î¡Î‘Î“Î©Î“Î—_(.+)\.csv', filename)
#         if not m:
#             log("timologia_weekly", f"Bad filename: {filename}")
#             continue
#         company_key = m.group(1)

#         prod_row = producers_df[producers_df['normalized_name'] == normalize_name(company_key)]
#         if prod_row.empty:
#             continue

#         company_name = str(prod_row['Î•Ï„Î±Î¹ÏÎµÎ¯Î±'].values[0])
#         print(f"\n=== Î•Ï€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± {filename} ({company_name}) ===")

#         df = read_production_data(str(file_path))
#         if df is None:
#             print("  -> SKIP: read_production_data ÎµÏ€Î­ÏƒÏ„ÏÎµÏˆÎµ None")
#             continue

#         df_week, summary = calculate_weekly_summary_from_month(
#             df, df_dam_month, prod_row, month_str, start_date, end_date
#         )
#         if df_week is None:
#             print("  -> SKIP: calculate_weekly_summary_from_month ÎµÏ€Î­ÏƒÏ„ÏÎµÏˆÎµ None")
#             continue

#         xlsx_path, company_name, email_value = generate_invoice_excel_weekly(
#             df_week, summary, prod_row, start_date, end_date, xlsx_dir, tag
#         )
#         if not xlsx_path:
#             print("  -> SKIP: generate_invoice_excel_weekly Î±Ï€Î­Ï„Ï…Ï‡Îµ")
#             continue

#         email_key  = (email_value or "NO_EMAIL").strip() or "NO_EMAIL"
#         subfolder  = determine_pdf_subfolder_name(email_key, email_to_companies)
#         target_dir = pdf_dir / subfolder[:MAX_FOLDER_CHARS]
#         target_dir.mkdir(parents=True, exist_ok=True)

#         pdf_name = pdf_filename_weekly(company_name, tag)
#         pdf_path = target_dir / pdf_name

#         ok, how = export_to_pdf(xlsx_path, str(pdf_path))
#         status   = "âœ… PDF" if ok else "âŒ PDF"
#         print(f"  {status} [{how}] â†’ {pdf_path}")

#     print(f"\nÎˆÏ„Î¿Î¹Î¼Î¿. Î”ÎµÏ‚: {root}/XLSX ÎºÎ±Î¹ {root}/PDF")

def timologia_weekly(start_date_str: str, end_date_str: str):
    # Parse dates
    try:
        start_date = pd.to_datetime(start_date_str).floor('D')
        end_date   = pd.to_datetime(end_date_str).floor('D')
    except Exception:
        print("ÎœÎ· Î­Î³ÎºÏ…ÏÎµÏ‚ Î·Î¼ÎµÏÎ¿Î¼Î·Î½Î¯ÎµÏ‚. Î§ÏÎ®ÏƒÎ·: YYYY-MM-DD")
        return

    if end_date < start_date:
        print("Î¤Î¿ Ï„Î­Î»Î¿Ï‚ ÎµÎ¯Î½Î±Î¹ Ï€ÏÎ¹Î½ Ï„Î·Î½ Î±ÏÏ‡Î®.")
        return

    if start_date.year != end_date.year or start_date.month != end_date.month:
        print("Î ÏÎ¿Ï‚ Ï„Î¿ Ï€Î±ÏÏŒÎ½ Î· ÎµÎ²Î´Î¿Î¼Î¬Î´Î± Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± ÎµÎ¯Î½Î±Î¹ Î¼Î­ÏƒÎ± ÏƒÏ„Î¿Î½ Î¯Î´Î¹Î¿ Î¼Î®Î½Î±.")
        print("Î§ÏÏÎ¹ÏƒÎ­ Ï„Î·Î½ ÏƒÎµ Î´ÏÎ¿ ÎºÎ»Î®ÏƒÎµÎ¹Ï‚ (Î¼Î¯Î± Î³Î¹Î± ÎºÎ¬Î¸Îµ Î¼Î®Î½Î±).")
        return

    month_str = start_date.strftime('%Y-%m')

    # 1) Î§Ï„Î¯Î¶Î¿Ï…Î¼Îµ/ÎµÎ½Î·Î¼ÎµÏÏÎ½Î¿Ï…Î¼Îµ Î Î‘Î¡Î‘Î“Î©Î“Î— Î±Ï€ÏŒ GREEN_VE6
    # ensure_production_files(start_date, end_date)

    # 2) Î¦Î¬ÎºÎµÎ»Î¿Î¹ ÎµÎ¾ÏŒÎ´Î¿Ï…
    tag, root, xlsx_dir, pdf_dir = make_week_dirs(start_date, end_date)

    # 3) Producers (Î£Î—Î˜Î¥Î‘ Î¼ÏŒÎ½Î¿)
    producers_df = load_producers_sithya(PRODUCERS_XLSX)
    if producers_df is None or producers_df.empty:
        print("Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ Ï€Î±ÏÎ±Î³Ï‰Î³Î¿Î¯ Î£Î—Î˜Î¥Î‘.")
        return

    # Ï‡ÏÎ®ÏƒÎ¹Î¼Î¿ Î³Î¹Î± grouping pdf ÏƒÎµ subfolders Î±Î½Î¬ email
    email_to_companies, _ = build_email_groups(producers_df)

    # 4) DAM 15' Î³Î¹Î± Ï„Î¿Î½ Î¼Î®Î½Î±
    if not DAM_FILE_2025.exists():
        print(f"Î›ÎµÎ¯Ï€ÎµÎ¹ DAM CSV: {DAM_FILE_2025.name}")
        return

    df_dam_month = load_dam_quarterly_endtime(str(DAM_FILE_2025), month_str)
    if df_dam_month is None or df_dam_month.empty:
        print("Î‘Ï€Î¿Ï„Ï…Ï‡Î¯Î±: DAM 15' prices (empty).")
        return

    # 5) Î Î±ÏÎ±Î³Ï‰Î³Î®: Ï‡ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹ PROD_DIR
    if not PROD_DIR.is_dir():
        print(f"Î›ÎµÎ¯Ï€ÎµÎ¹ Ï†Î¬ÎºÎµÎ»Î¿Ï‚: {PROD_DIR} (ÎºÎ±Î¹ Î´ÎµÎ½ Î¼Ï€ÏŒÏÎµÏƒÎ± Î½Î± Ï„Î¿Î½ Ï†Ï„Î¹Î¬Î¾Ï‰ Î±Ï€ÏŒ downloads)")
        return
    
    def safe_company_folder_name(name: str) -> str:
        # 1. Unicode normalization
        name = unicodedata.normalize("NFKC", name)

        # 2. Î‘Î½Ï„Î¹ÎºÎ±Ï„Î¬ÏƒÏ„Î±ÏƒÎ· ÎŸÎ›Î©Î Ï„Ï‰Î½ whitespace (space, NBSP, tabs ÎºÎ»Ï€) Î¼Îµ _
        name = re.sub(r"\s+", "_", name)

        # 3. ÎšÎ±Î¸Î¬ÏÎ¹ÏƒÎ¼Î± Î´Î¹Ï€Î»ÏÎ½ _
        name = re.sub(r"_+", "_", name)

        # 4. Trim
        return name.strip("_")

    # ------------------------------------------------------------
    # Helper: Î²ÏÎµÏ‚ Ï„Î¿ ÏƒÏ‰ÏƒÏ„ÏŒ production CSV Î³Î¹Î± Î­Î½Î±Î½ Ï€Î±ÏÎ±Î³Ï‰Î³ÏŒ
    # (Ï€ÏÏÏ„Î± Î½Î­Î¿ path, Î¼ÎµÏ„Î¬ fallback ÏƒÎµ Ï€Î±Î»Î¹ÏŒ flat path)
    # ------------------------------------------------------------
    def _find_production_file_for_producer(company_name: str):
        """
        Returns Path or None
        - New structure: PROD_DIR/{Î•Î¤Î‘Î™Î¡Î•Î™Î‘}/Î Î‘Î¡Î‘Î“Î©Î“Î—_{Î•Î¤Î‘Î™Î¡Î•Î™Î‘}.csv
        - Old structure: PROD_DIR/Î Î‘Î¡Î‘Î“Î©Î“Î—_{something}.csv (normalize match)
        """
        comp = safe_company_folder_name(company_name)

        # Fallback: old structure - try direct name
        p_old_direct = PROD_DIR / f"Î Î‘Î¡Î‘Î“Î©Î“Î—_{comp}.csv"
        if p_old_direct.exists():
            return p_old_direct

        # Fallback: scan PROD_DIR for a match by normalized name
        comp_norm = normalize_name(comp)
        try:
            for fn in os.listdir(PROD_DIR):
                if not (fn.startswith("Î Î‘Î¡Î‘Î“Î©Î“Î—_") and fn.endswith(".csv")):
                    continue
                m = re.match(r'Î Î‘Î¡Î‘Î“Î©Î“Î—_(.+)\.csv', fn)
                if not m:
                    continue
                key = m.group(1)
                if normalize_name(key) == comp_norm:
                    return PROD_DIR / fn
        except Exception:
            pass

        return None

    # ------------------------------------------------------------
    # ÎšÏÏÎ¹Î¿ loop: Ï€Î¬Î½Ï‰ ÏƒÏ„Î¿Ï…Ï‚ Ï€Î±ÏÎ±Î³Ï‰Î³Î¿ÏÏ‚ Î£Î—Î˜Î¥Î‘ (producers_df)
    # ------------------------------------------------------------
    processed = 0
    skipped_no_file = 0
    skipped_errors = 0

    # Î‘Î½ Î´ÎµÎ½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ normalized_name, Ï„Î¿ Ï†Ï„Î¹Î¬Ï‡Î½Î¿Ï…Î¼Îµ Ï€ÏÏŒÏ‡ÎµÎ¹ÏÎ±
    if "normalized_name" not in producers_df.columns:
        producers_df = producers_df.copy()
        producers_df["normalized_name"] = producers_df["Î•Ï„Î±Î¹ÏÎµÎ¯Î±"].astype(str).apply(normalize_name)

    for _, prod_row_series in producers_df.iterrows():
        # ÎºÎ¬Î½Î¿Ï…Î¼Îµ prod_row DataFrame 1-Î³ÏÎ±Î¼Î¼Î®Ï‚ Î³Î¹Î± Î½Î± Ï„Î±Î¹ÏÎ¹Î¬Î¶ÎµÎ¹ Î¼Îµ Ï„Î¹Ï‚ Ï…Ï€Î¬ÏÏ‡Î¿Ï…ÏƒÎµÏ‚ ÏƒÏ…Î½Î±ÏÏ„Î®ÏƒÎµÎ¹Ï‚ ÏƒÎ¿Ï…
        prod_row = producers_df.loc[[prod_row_series.name]]

        company_name = str(prod_row_series.get("Î•Ï„Î±Î¹ÏÎµÎ¯Î±", "")).strip()
        if not company_name:
            continue

        file_path = _find_production_file_for_producer(company_name)
        if file_path is None or not file_path.exists():
            skipped_no_file += 1
            print(f"\n=== {company_name} ===")
            print("  -> SKIP: Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ Î±ÏÏ‡ÎµÎ¯Î¿ Ï€Î±ÏÎ±Î³Ï‰Î³Î®Ï‚ (Î Î‘Î¡Î‘Î“Î©Î“Î—_*.csv)")
            continue

        print(f"\n=== Î•Ï€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± Ï€Î±ÏÎ±Î³Ï‰Î³Î¿Ï: {company_name} ===")
        print(f"  Production file: {file_path.name}")

        df = read_production_data(str(file_path))
        if df is None:
            skipped_errors += 1
            print("  -> SKIP: read_production_data ÎµÏ€Î­ÏƒÏ„ÏÎµÏˆÎµ None")
            continue

        df_week, summary = calculate_weekly_summary_from_month(
            df, df_dam_month, prod_row, month_str, start_date, end_date
        )
        if df_week is None:
            skipped_errors += 1
            print("  -> SKIP: calculate_weekly_summary_from_month ÎµÏ€Î­ÏƒÏ„ÏÎµÏˆÎµ None")
            continue

        xlsx_path, company_name_out, email_value = generate_invoice_excel_weekly(
            df_week, summary, prod_row, start_date, end_date, xlsx_dir, tag
        )
        if not xlsx_path:
            skipped_errors += 1
            print("  -> SKIP: generate_invoice_excel_weekly Î±Ï€Î­Ï„Ï…Ï‡Îµ")
            continue

        # PDF export per email group subfolder
        email_key  = (email_value or "NO_EMAIL").strip() or "NO_EMAIL"
        subfolder  = determine_pdf_subfolder_name(email_key, email_to_companies)
        target_dir = pdf_dir / subfolder[:MAX_FOLDER_CHARS]
        target_dir.mkdir(parents=True, exist_ok=True)

        pdf_name = pdf_filename_weekly(company_name_out, tag)
        pdf_path = target_dir / pdf_name

        ok, how = export_to_pdf(xlsx_path, str(pdf_path))
        status  = "âœ… PDF" if ok else "âŒ PDF"
        print(f"  {status} [{how}] â†’ {pdf_path}")

        processed += 1

    print("\n" + "=" * 70)
    print(f"ÎŸÎ»Î¿ÎºÎ»Î·ÏÏÎ¸Î·ÎºÎµ.")
    print(f"Processed: {processed}")
    print(f"Skipped (no production file): {skipped_no_file}")
    print(f"Skipped (errors): {skipped_errors}")
    print(f"Î”ÎµÏ‚: {root}/XLSX ÎºÎ±Î¹ {root}/PDF")


if __name__ == "__main__":
    start = input("Î”ÏÏƒÎµ Î±ÏÏ‡Î® ÎµÎ²Î´Î¿Î¼Î¬Î´Î±Ï‚ (YYYY-MM-DD): ").strip()
    end   = input("Î”ÏÏƒÎµ Ï„Î­Î»Î¿Ï‚ ÎµÎ²Î´Î¿Î¼Î¬Î´Î±Ï‚ (YYYY-MM-DD): ").strip()
    timologia_weekly(start, end)